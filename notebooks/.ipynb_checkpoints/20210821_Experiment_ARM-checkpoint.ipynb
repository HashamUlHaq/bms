{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment and Spark Session setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desired SparkNLP Version: 3.1.3\n",
      "Desired SparkNLP-JSL Version: 3.1.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from ctdi_treatment.env_setup_start import *\n",
    "import time\n",
    "license_keys = set_envvars('/home/ubuntu/hasham/jsl_keys.json')\n",
    "spark = start_sparknlp(license_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the joined `design_group` + `intervention` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "alldf = pd.read_csv(\"../data/raw_aac.csv\")\n",
    "print (alldf.columns)\n",
    "alldf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also calculating the fields that represent the actual input to the pipeline\n",
    "#### Concatenation of ARM Title and Desc, Intervention Name and Desc,  Intervention Type, Name and Desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_sep = \": \"\n",
    "sent_sep = \".\"\n",
    "tit_sep = \"\\n\\n\"\n",
    "\n",
    "alldf[\"arm_title_desc\"] = alldf[\"title\"]+tit_sep+alldf[\"arm_desc\"]\n",
    "alldf[\"intervention_name_desc\"] = alldf[\"name\"]+name_sep+alldf[\"intervention_desc\"]+sent_sep\n",
    "alldf[\"intervention_type_name_desc\"] = alldf[\"intervention_type\"]+tit_sep+alldf[\"intervention_name_desc\"]\n",
    "alldf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_df = alldf.groupby([\"nct_id\",\"design_group_id\",\"title\",\"arm_desc\"]).agg(\n",
    "    intervention_type_name_desc=(\"intervention_type_name_desc\", tit_sep.join)\n",
    ").reset_index()\n",
    "\n",
    "# Exps Quadruplets: 1. DataFrame, 2. PipelineField, 3. AACT fallback field for drugs\n",
    "exps = [\n",
    "    (alldf[[\"nct_id\",\"design_group_id\",\"title\",\"arm_desc\",\"arm_title_desc\"]].drop_duplicates(), \"arm_title_desc\", \"title\"),\n",
    "    (alldf[[\"nct_id\",\"design_group_id\",\"title\",\"arm_desc\",\"arm_title_desc\",\"name\",\"intervention_type\",\"intervention_name_desc\"]].drop_duplicates(), \"intervention_name_desc\", \"name\"),\n",
    "    (annotation_df, \"intervention_type_name_desc\", \"name\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building or Loading the Spark Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctdi_treatment.resolver_pipeline import ResolverPipeline\n",
    "resolver_pipeline = ResolverPipeline(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[OK!]\n",
      "pos_clinical download started this may take some time.\n",
      "Approximate size to download 1.5 MB\n",
      "[OK!]\n",
      "dependency_conllu download started this may take some time.\n",
      "Approximate size to download 16.7 MB\n",
      "[OK!]\n",
      "ner_posology download started this may take some time.\n",
      "Approximate size to download 13.8 MB\n",
      "[OK!]\n",
      "ner_clinical download started this may take some time.\n",
      "Approximate size to download 13.9 MB\n",
      "[OK!]\n",
      "ner_clinical_large download started this may take some time.\n",
      "Approximate size to download 13.9 MB\n",
      "[OK!]\n",
      "ner_jsl download started this may take some time.\n",
      "Approximate size to download 14.5 MB\n",
      "[OK!]\n",
      "assertion_dl download started this may take some time.\n",
      "Approximate size to download 1.3 MB\n",
      "[OK!]\n",
      "sbiobert_base_cased_mli download started this may take some time.\n",
      "Approximate size to download 384.3 MB\n",
      "[OK!]\n",
      "sbiobertresolve_rxnorm download started this may take some time.\n",
      "Approximate size to download 802.6 MB\n",
      "[OK!]\n",
      "71.05825638771057\n"
     ]
    }
   ],
   "source": [
    "from ctdi_treatment.treatment_pipeline import build_treatment_pipeline, build_df, LightPipeline, PipelineModel\n",
    "pl_name = \"20210722_ner_pl_from_arms\"\n",
    "build_or_load = \"build\"#\"load\"\n",
    "s = time.time()\n",
    "if build_or_load==\"build\":\n",
    "    pl = build_treatment_pipeline()\n",
    "    plm = pl.fit(spark.createDataFrame([(\"\",)]).toDF(\"text\"))\n",
    "else:\n",
    "    plm = PipelineModel.load(f\"models/{pl_name}\")\n",
    "print(time.time()-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pipeline = False\n",
    "if save_pipeline:\n",
    "    s = time.time()\n",
    "    plm.write().overwrite().save(f\"models/{pl_name}\")\n",
    "    print(time.time()-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating SparkNLP LightPipeline to avoid Spark overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpl = LightPipeline(plm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.88661432266235\n"
     ]
    }
   ],
   "source": [
    "use_light_pipeline = True\n",
    "s = time.time()\n",
    "dfs = []\n",
    "for e in exps:\n",
    "    if use_light_pipeline:\n",
    "        dfs.append((e[-1],build_df(*e[:-1], lpl)))\n",
    "    else:\n",
    "        plm.stages[0].setInputCol(e[1])\n",
    "        # These line here makes no sense in reality but just for reference if you were to use a cluster\n",
    "        dfs.append((e[-1],plm.transform(spark.createDataFrame(e[0])).toPandas()))\n",
    "print(time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 32)\n",
      "(45, 35)\n",
      "(13, 32)\n"
     ]
    }
   ],
   "source": [
    "for _,o in dfs:\n",
    "    print(o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running postprocessing for all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import mode\n",
    "from ctdi_treatment.postprocessing import build_dict_acc, prepare_output_acc, aggregate_entity_dict, dict_diff_acc, dict_join_append_acc\n",
    "for i, (ff,o) in enumerate(dfs):\n",
    "    dfs[i][1][\"entity_dict\"] = o[\"full_chunk\"].apply(build_dict_acc(by_sentence=False))\n",
    "    dfs[i][1][\"entity_dict_sent\"] = o[\"full_chunk\"].apply(build_dict_acc())\n",
    "    dfs[i][1][\"num_sents\"] = o[\"sentence\"].apply(len)\n",
    "    dfs[i][1][\"num_drugs\"] = o[\"entity_dict\"].apply(lambda x: len(x.get(\"Drug\",[])))\n",
    "    dfs[i][1][\"output\"] = o.apply(prepare_output_acc(fallback_field=ff, name_sep=name_sep, sent_sep=sent_sep, tit_sep=tit_sep), axis=1)\n",
    "    dfs[i][1][['output_class', 'missing_entities', 'output']] = pd.DataFrame(o[\"output\"].tolist(), index=o.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_,dfx), (_,dfz), (_,dfa) = dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ARM and Intervention level information together and calculate `combined` final output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation of the Intervention level output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 11)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfz_ = dfz.groupby([\"nct_id\",\"design_group_id\"])\\\n",
    "    .agg({\"name\":name_sep.join,\"title\":max,\"arm_desc\":max,\n",
    "           \"intervention_name_desc\":tit_sep.join,\"entity_dict\":aggregate_entity_dict,\n",
    "          \"output_class\":lambda x: mode(x)[0],\"missing_entities\":sum,\"output\":sum,\"num_drugs\":sum}).reset_index()\n",
    "dfz_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm_int = pd.merge(dfx,dfz.drop([\"nct_id\",\"title\",\"arm_desc\",\"arm_title_desc\"],axis=1),on=\"design_group_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "required = [\"Drug\",\"Strength\",\"Administration\"]\n",
    "arm_int[\"entity_diff_xy\"] = arm_int[[\"entity_dict_x\",\"entity_dict_y\"]].apply(dict_diff_acc(False,required), axis=1)\n",
    "arm_int[\"label_diff_xy\"] = arm_int[[\"entity_dict_x\",\"entity_dict_y\"]].apply(dict_diff_acc(True,required), axis=1)\n",
    "arm_int[\"labels_x\"] = arm_int[\"entity_dict_x\"].apply(lambda x: [y for y in x])\n",
    "arm_int[\"labels_y\"] = arm_int[\"entity_dict_y\"].apply(lambda x: [y for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm_int[\"output_read_x\"] = arm_int[\"output_x\"].apply(lambda x: \"\\n\".join([str(y) for y in x]))\n",
    "arm_int[\"output_read_y\"] = arm_int[\"output_y\"].apply(lambda x: \"\\n\".join([str(y) for y in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm_int[\"combined\"] = arm_int.apply(dict_join_append_acc(),axis=1)\n",
    "arm_int[\"combined_read\"] = arm_int[\"combined\"].apply(lambda x: \"\\n\".join([str(y) for y in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "arm_int[[\"nct_id\",\"title\",\"arm_title_desc\",\"name\",\"intervention_name_desc\",\"output_class_x\",\"output_class_y\",\"output_x\",\"output_y\",\"entity_diff_xy\",\"combined\",\"output_read_x\",\"output_read_y\",\"combined_read\"]]\\\n",
    ".to_csv(\"../data/arm_title_desc_int_name_desc_combined.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some preannotated data for the AnnotationLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_df = pd.merge(dfx,dfa.drop([\"nct_id\",\"title\",\"arm_desc\"],axis=1),on=\"design_group_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctdi_treatment.annotation_lab import write_json_files\n",
    "write_json_files(ann_df, [\"nct_id\",\"title\"],[(\"arm\",\"arm_title_desc\",\"full_chunk_x\",\"relations_x\"),\n",
    "                  (\"int\",\"intervention_type_name_desc\",\"full_chunk_y\",\"relations_y\")], out_path=\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_anaconda3)",
   "language": "python",
   "name": "conda_anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
